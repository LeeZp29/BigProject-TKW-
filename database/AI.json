{
    "researchs":[
        {
            "name": "A review of possible effects of cognitive biases on interpretation of rule-based machine learning models",
            "author":"Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz",
            "review-article":"While the interpretability of machine learning models is often equated with their mere syntactic comprehensibility, we think that interpretability goes beyond that, and that human interpretability should also be investigated from the point of view of cognitive science. The goal of this paper is to discuss to what extent cognitive biases may affect human understanding of interpretable machine learning models, in particular of logical rules discovered from data. Twenty cognitive biases are covered, as are possible debiasing techniques that can be adopted by designers of machine learning algorithms and software. Our review transfers results obtained in cognitive psychology to the domain of machine learning, aiming to bridge the current gap between these two areas. It needs to be followed by empirical studies specifically focused on the machine learning domain.",
            "lastest-update":"June 2021",
            "research-link":"https://www.sciencedirect.com/science/article/pii/S0004370221000096",
            "pdf":"https://www.sciencedirect.com/science/article/pii/S0004370221000096/pdfft?md5=67b3a0c5e6c59bd6167b0e6d9e584f3f&pid=1-s2.0-S0004370221000096-main.pdf"
        },
        {
            "name": "Hard choices in artificial intelligence",
            "author":"Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz",
            "review-article":"As AI systems are integrated into high stakes social domains, researchers now examine how to design and operate them in a safe and ethical manner. However, the criteria for identifying and diagnosing safety risks in complex social contexts remain unclear and contested. In this paper, we examine the vagueness in debates about the safety and ethical behavior of AI systems. We show how this vagueness cannot be resolved through mathematical formalism alone, instead requiring deliberation about the politics of development as well as the context of deployment. Drawing from a new sociotechnical lexicon, we redefine vagueness in terms of distinct design challenges at key stages in AI system development. The resulting framework of Hard Choices in Artificial Intelligence (HCAI) empowers developers by 1) identifying points of overlap between design decisions and major sociotechnical challenges; 2) motivating the creation of stakeholder feedback channels so that safety issues can be exhaustively addressed. As such, HCAI contributes to a timely debate about the status of AI development in democratic societies, arguing that deliberation should be the goal of AI Safety, not just the procedure by which it is ensured.",
            "lastest-update":"November 2021",
            "research-link":"https://www.sciencedirect.com/science/article/pii/S0004370221001065",
            "pdf":"https://www.sciencedirect.com/science/article/pii/S0004370221001065/pdfft?md5=8c907c74fdf709f74276185836b57598&pid=1-s2.0-S0004370221001065-main.pdf"
        },
        {
            "name": "Making sense of sensory input",
            "author":"Richard Evans, José Hernández-Orallo, Johannes Welbl, Pushmeet Kohli, Marek Sergot",
            "review-article":"This paper attempts to answer a central question in unsupervised learning: what does it mean to “make sense” of a sensory sequence? In our formalization, making sense involves constructing a symbolic causal theory that both explains the sensory sequence and also satisfies a set of unity conditions. The unity conditions insist that the constituents of the causal theory objects, properties, and laws must be integrated into a coherent whole. On our account, making sense of sensory input is a type of program synthesis, but it is unsupervised program synthesis.<br><br>Our second contribution is a computer implementation, the Apperception Engine, that was designed to satisfy the above requirements. Our system is able to produce interpretable human-readable causal theories from very small amounts of data, because of the strong inductive bias provided by the unity conditions. A causal theory produced by our system is able to predict future sensor readings, as well as retrodict earlier readings, and impute (fill in the blanks of) missing sensory readings, in any combination. In fact, it is able to do all three tasks simultaneously.<br><br>We tested the engine in a diverse variety of domains, including cellular automata, rhythms and simple nursery tunes, multi-modal binding problems, occlusion tasks, and sequence induction intelligence tests. In each domain, we test our engine's ability to predict future sensor values, retrodict earlier sensor values, and impute missing sensory data. The Apperception Engine performs well in all these domains, significantly out-performing neural net baselines. We note in particular that in the sequence induction intelligence tests, our system achieved human-level performance. This is notable because our system is not a bespoke system designed specifically to solve intelligence tests, but a general-purpose system that was designed to make sense of any sensory sequence.",
            "lastest-update":"April 2021",
            "research-link":"https://www.sciencedirect.com/science/article/pii/S0004370220301855",
            "pdf":"https://www.sciencedirect.com/science/article/pii/S0004370220301855/pdfft?md5=da7b5b3343a130e065db0c8b888e16a0&pid=1-s2.0-S0004370220301855-main.pdf"
        },
        {
            "name": "Making sense of raw input",
            "author":"Richard Evans, Matko Bošnjak, Lars Buesing, Kevin Ellis, David Pfau, Pushmeet Kohli, Marek Sergot",
            "review-article":"How should a machine intelligence perform unsupervised structure discovery over streams of sensory input? One approach to this problem is to cast it as an apperception task. Here, the task is to construct an explicit interpretable theory that both explains the sensory sequence and also satisfies a set of unity conditions, designed to ensure that the constituents of the theory are connected in a relational structure.<br><br>However, the original formulation of the apperception task had one fundamental limitation: it assumed the raw sensory input had already been parsed using a set of discrete categories, so that all the system had to do was receive this already-digested symbolic input, and make sense of it. But what if we don't have access to pre-parsed input? What if our sensory sequence is raw unprocessed information?<br<br>The central contribution of this paper is a neuro-symbolic framework for distilling interpretable theories out of streams of raw, unprocessed sensory experience. First, we extend the definition of the apperception task to include ambiguous (but still symbolic) input: sequences of sets of disjunctions. Next, we use a neural network to map raw sensory input to disjunctive input. Our binary neural network is encoded as a logic program, so the weights of the network and the rules of the theory can be solved jointly as a single SAT problem. This way, we are able to jointly learn how to perceive (mapping raw sensory information to concepts) and apperceive (combining concepts into declarative rules).",
            "lastest-update":"October 2021",
            "research-link":"https://www.sciencedirect.com/science/article/pii/S0004370221000722",
            "pdf":"https://www.sciencedirect.com/science/article/pii/S0004370221000722/pdfft?md5=1c1af40d88a499b2aa305de762ec629d&pid=1-s2.0-S0004370221000722-main.pdf"
        }
    ]
}